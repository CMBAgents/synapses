#!/usr/bin/env python3
"""
Script unifiÃ© pour la gestion complÃ¨te des contextes
Remplace: generate-missing-contexts.py, generate-contexts-with-clone.py, 
         update-json-status.py, verify-context-structure.py, cleanup-duplicate-contexts.py,
         et les fonctionnalitÃ©s de manage-contexts.py

FonctionnalitÃ©s unifiÃ©es:
1. âœ… VÃ©rification de la structure des contextes
2. âœ… GÃ©nÃ©ration des contextes manquants avec clonage Git
3. âœ… Nettoyage des doublons (racine et avancÃ©)
4. âœ… Mise Ã  jour des statuts JSON
5. âœ… GÃ©nÃ©ration du module embedded-context.ts
6. âœ… RÃ©gÃ©nÃ©ration de config.json
7. âœ… Gestion des hashes et dÃ©tection des changements
"""

import os
import sys
import json
import csv
import hashlib
import subprocess
import tempfile
import shutil
import logging
import time
import requests
from pathlib import Path
from datetime import datetime, timedelta
from typing import Dict, List, Optional, Set, Tuple

class UnifiedContextManager:
    def __init__(self, base_dir: str = "."):
        self.base_dir = Path(base_dir)
        self.data_dir = self.base_dir / "app" / "data"
        self.context_dir = self.base_dir / "public" / "context"
        self.app_context_dir = self.base_dir / "app" / "context"
        self.config_file = self.base_dir / "config.json"
        self.state_file = self.base_dir / "context_manager_state.json"
        self.temp_dir = self.base_dir / "temp" / "repos"
        
        # CrÃ©er les dossiers nÃ©cessaires
        self.temp_dir.mkdir(parents=True, exist_ok=True)
        
        # Setup logging
        self.setup_logging()
        
        # Load state
        self.state = self.load_state()
        
        # Configuration des domaines - chargÃ©e depuis domain-config.json
        self.domains = self.load_domains_config()
        
    def setup_logging(self):
        """Configure le logging"""
        # CrÃ©er le dossier logs s'il n'existe pas
        logs_dir = self.base_dir / 'logs'
        logs_dir.mkdir(exist_ok=True)
        
        # Effacer le fichier de log prÃ©cÃ©dent pour Ã©viter l'accumulation
        log_file = logs_dir / 'unified_context_manager.log'
        if log_file.exists():
            log_file.unlink()
            print(f"ğŸ§¹ Fichier de log prÃ©cÃ©dent effacÃ©: {log_file}")
        
        logging.basicConfig(
            level=logging.INFO,
            format='%(asctime)s - %(levelname)s - %(message)s',
            handlers=[
                logging.FileHandler(str(log_file)),
                logging.StreamHandler()
            ]
        )
        self.logger = logging.getLogger(__name__)
    
    def load_state(self) -> Dict:
        """Charge l'Ã©tat prÃ©cÃ©dent"""
        if self.state_file.exists():
            try:
                with open(self.state_file, 'r') as f:
                    return json.load(f)
            except Exception:
                pass
        
        return {
            "last_check": None,
            "context_hashes": {},
            "last_update": None,
            "update_count": 0
        }
    
    def save_state(self):
        """Sauvegarde l'Ã©tat actuel"""
        try:
            with open(self.state_file, 'w') as f:
                json.dump(self.state, f, indent=2)
        except Exception as e:
            self.logger.error(f"Erreur sauvegarde Ã©tat: {e}")
    
    def load_domains_config(self) -> List[str]:
        """Charge la configuration des domaines depuis config.json"""
        try:
            config_file = self.base_dir / "config.json"
            if config_file.exists():
                with open(config_file, 'r', encoding='utf-8') as f:
                    config = json.load(f)
                    domains = config.get('domains', {}).get('supported', [])
                    self.logger.info(f"Domaines chargÃ©s depuis config.json: {domains}")
                    return domains
            else:
                self.logger.warning("Fichier config.json non trouvÃ©, utilisation des domaines par dÃ©faut")
                return ["astronomy", "finance", "biochemistry", "machinelearning"]
        except Exception as e:
            self.logger.error(f"Erreur chargement configuration domaines: {e}")
            return ["astronomy", "finance", "biochemistry", "machinelearning"]
    
    def get_file_hash(self, filepath: Path) -> str:
        """Calcule le hash MD5 d'un fichier"""
        if not filepath.exists():
            return ""
        
        hash_md5 = hashlib.md5()
        try:
            with open(filepath, "rb") as f:
                for chunk in iter(lambda: f.read(4096), b""):
                    hash_md5.update(chunk)
            return hash_md5.hexdigest()
        except Exception as e:
            self.logger.error(f"Erreur calcul hash {filepath}: {e}")
            return ""

    # ============================================================================
    # 1. VÃ‰RIFICATION DE LA STRUCTURE DES CONTEXTES
    # ============================================================================
    
    def verify_context_structure(self) -> bool:
        """VÃ©rifie que les fichiers de contexte sont dans la bonne structure"""
        self.logger.info("ğŸ” VÃ©rification de la structure des contextes...")
        
        if not self.context_dir.exists():
            self.logger.error("public/context directory does not exist")
            return False
        
        # VÃ©rifier les fichiers de contexte Ã  la racine (ne doivent pas exister)
        root_context_files = []
        for file in self.context_dir.iterdir():
            if file.is_file() and file.name.endswith('-context.txt'):
                root_context_files.append(file.name)
        
        if root_context_files:
            self.logger.error(f"âŒ Found {len(root_context_files)} context files in public/context root (should not exist):")
            for file in root_context_files:
                self.logger.error(f"  - {file}")
            return False
        
        # VÃ©rifier les sous-rÃ©pertoires de domaines
        total_contexts = 0
        
        for domain in self.domains:
            domain_dir = self.context_dir / domain
            if not domain_dir.exists():
                self.logger.warning(f"Domain directory {domain}/ does not exist")
                continue
                
            self.logger.info(f"Checking domain: {domain}")
            
            # Compter les fichiers de contexte dans le rÃ©pertoire de domaine
            domain_files = []
            for file in domain_dir.iterdir():
                if file.is_file() and file.name.endswith('-context.txt'):
                    domain_files.append(file.name)
            
            self.logger.info(f"  Found {len(domain_files)} context files in {domain}/")
            total_contexts += len(domain_files)
        
        self.logger.info(f"âœ… Structure verification completed. Total contexts: {total_contexts}")
        return True

    # ============================================================================
    # 2. GÃ‰NÃ‰RATION DES CONTEXTES MANQUANTS
    # ============================================================================
    
    def check_contextmaker_available(self) -> bool:
        """VÃ©rifie si contextmaker est disponible"""
        try:
            import contextmaker
            # VÃ©rifier la version et forcer la mise Ã  jour si nÃ©cessaire
            import subprocess
            try:
                result = subprocess.run(
                    ["pip3", "install", "--upgrade", "contextmaker"],
                    capture_output=True,
                    text=True,
                    timeout=120
                )
                if result.returncode == 0:
                    self.logger.info("âœ… contextmaker mis Ã  jour vers la derniÃ¨re version")
                else:
                    self.logger.warning(f"âš ï¸ Mise Ã  jour de contextmaker Ã©chouÃ©e: {result.stderr}")
            except Exception as e:
                self.logger.warning(f"âš ï¸ Impossible de mettre Ã  jour contextmaker: {e}")
            
            return True
        except ImportError:
            return False
    
    def check_git_available(self) -> bool:
        """VÃ©rifie si git est disponible"""
        try:
            result = subprocess.run(
                ["git", "--version"],
                capture_output=True,
                text=True
            )
            return result.returncode == 0
        except FileNotFoundError:
            return False
    
    def get_github_latest_commit(self, repo_url: str) -> Optional[str]:
        """RÃ©cupÃ¨re le dernier commit SHA d'un repository GitHub"""
        try:
            # Convertir l'URL GitHub en format API
            if "github.com" in repo_url:
                repo_path = repo_url.replace(".git", "").replace("https://github.com/", "")
                api_url = f"https://api.github.com/repos/{repo_path}/commits"
                
                response = requests.get(api_url, timeout=10)
                if response.status_code == 200:
                    commits = response.json()
                    if commits and len(commits) > 0:
                        return commits[0]["sha"]
                else:
                    self.logger.warning(f"API GitHub error for {repo_url}: {response.status_code}")
            return None
        except Exception as e:
            self.logger.warning(f"Erreur rÃ©cupÃ©ration commit GitHub {repo_url}: {e}")
            return None
    
    def check_repo_has_changed(self, library: Dict) -> bool:
        """VÃ©rifie si un repository a changÃ© depuis la derniÃ¨re gÃ©nÃ©ration de contexte"""
        try:
            # RÃ©cupÃ©rer l'URL GitHub
            github_url = library.get('github_url', '')
            if not github_url:
                return False
            
            # RÃ©cupÃ©rer le dernier commit SHA
            latest_commit = self.get_github_latest_commit(github_url)
            if not latest_commit:
                return False
            
            # VÃ©rifier si on a dÃ©jÃ  stockÃ© le commit pour cette librairie
            lib_name = library.get('name', '')
            stored_commit = self.state.get('repo_commits', {}).get(lib_name)
            
            if stored_commit != latest_commit:
                # Le repo a changÃ©, mettre Ã  jour le commit stockÃ©
                if 'repo_commits' not in self.state:
                    self.state['repo_commits'] = {}
                self.state['repo_commits'][lib_name] = latest_commit
                self.save_state()
                return True
            
            return False
            
        except Exception as e:
            self.logger.warning(f"Erreur vÃ©rification changement repo {library.get('name', '')}: {e}")
            return False
    
    def mark_outdated_contexts_as_missing(self):
        """Marque les contextes obsolÃ¨tes comme manquants pour forcer leur rÃ©gÃ©nÃ©ration"""
        self.logger.info("ğŸ” VÃ©rification des modifications des repositories GitHub...")
        
        try:
            domains = ['astronomy', 'biochemistry', 'finance', 'machinelearning']
            total_checked = 0
            total_updated = 0
            
            for domain in domains:
                domain_data = self.load_domain_data(domain)
                if not domain_data or 'libraries' not in domain_data:
                    continue
                
                domain_updated = 0
                for library in domain_data['libraries']:
                    total_checked += 1
                    
                    # VÃ©rifier si le repo a changÃ©
                    if self.check_repo_has_changed(library):
                        # Marquer le contexte comme manquant
                        library['hasContextFile'] = False
                        if 'contextFileName' in library:
                            del library['contextFileName']
                        
                        domain_updated += 1
                        total_updated += 1
                        
                        lib_name = library.get('name', 'Unknown')
                        self.logger.info(f"  ğŸ”„ {lib_name}: Repository modifiÃ©, contexte marquÃ© comme manquant")
                
                # Sauvegarder le domaine si des modifications ont Ã©tÃ© faites
                if domain_updated > 0:
                    domain_file = self.data_dir / f"{domain}-libraries.json"
                    with open(domain_file, 'w') as f:
                        json.dump(domain_data, f, indent=2)
                    
                    self.logger.info(f"  âœ… {domain}: {domain_updated} contextes marquÃ©s comme manquants")
            
            self.logger.info(f"âœ… VÃ©rification terminÃ©e: {total_checked} repos vÃ©rifiÃ©s, {total_updated} contextes marquÃ©s comme manquants")
            
        except Exception as e:
            self.logger.error(f"âŒ Erreur vÃ©rification modifications repos: {e}")
    
    def clone_repository(self, owner: str, repo: str, package_name: str) -> bool:
        """Clone un repository GitHub"""
        try:
            repo_url = f"https://github.com/{owner}/{repo}.git"
            repo_dir = self.temp_dir / package_name
            
            if repo_dir.exists():
                shutil.rmtree(repo_dir)
            
            self.logger.info(f"ğŸ”„ Cloning {repo_url}...")
            
            result = subprocess.run([
                'git', 'clone', '--depth', '1', repo_url, str(repo_dir)
            ], capture_output=True, text=True, timeout=300)
            
            if result.returncode == 0:
                self.logger.info(f"âœ… Successfully cloned {repo_url}")
                return True
            else:
                self.logger.error(f"âŒ Failed to clone {repo_url}")
                self.logger.error(f"STDERR: {result.stderr}")
                return False
                
        except subprocess.TimeoutExpired:
            self.logger.error(f"âŒ Timeout cloning {repo_url}")
            return False
        except Exception as e:
            self.logger.error(f"âŒ Exception cloning {repo_url}: {e}")
            return False
    
    def generate_context_with_contextmaker(self, package_name: str, lib_name: str, domain: str) -> Optional[str]:
        """GÃ©nÃ¨re le contenu du contexte avec contextmaker"""
        try:
            repo_dir = self.temp_dir / package_name
            if not repo_dir.exists():
                self.logger.error(f"Repository directory not found: {repo_dir}")
                return None
            
            # Utiliser contextmaker.make() directement
            import contextmaker
            
            self.logger.info(f"ğŸ”„ Running contextmaker for {package_name}...")
            
            result = contextmaker.make(
                library_name=package_name,
                output_path=str(repo_dir),
                input_path=str(repo_dir),
                rough=True,
                extension='txt'
            )
            
            if result:
                # Le fichier gÃ©nÃ©rÃ© sera nommÃ© {package_name}.txt
                generated_file = repo_dir / f"{package_name}.txt"
                if generated_file.exists():
                    # Lire le contenu gÃ©nÃ©rÃ©
                    with open(generated_file, 'r', encoding='utf-8') as f:
                        content = f.read()
                    
                    # Nettoyer le fichier temporaire
                    generated_file.unlink()
                else:
                    self.logger.error(f"Generated file not found: {generated_file}")
                    return None
                
                self.logger.info(f"âœ… Context file generated for {package_name}")
                return content
            else:
                self.logger.error(f"âŒ Context file not created for {package_name}")
                return None
                
        except Exception as e:
            self.logger.error(f"âŒ Exception running contextmaker for {package_name}: {e}")
            return None
    
    def save_context_file(self, domain: str, lib_name: str, content: str) -> bool:
        """Sauvegarde le fichier de contexte"""
        try:
            domain_dir = self.context_dir / domain
            domain_dir.mkdir(parents=True, exist_ok=True)
            
            # Normaliser le nom de la bibliothÃ¨que
            normalized_name = lib_name.replace('/', '-').replace('_', '-')
            filename = f"{normalized_name}-context.txt"
            filepath = domain_dir / filename
            
            with open(filepath, 'w', encoding='utf-8') as f:
                f.write(content)
            
            self.logger.info(f"âœ… Context file saved: {filepath}")
            return True
            
        except Exception as e:
            self.logger.error(f"âŒ Error saving context file: {e}")
            return False
    
    def get_existing_contexts(self) -> Dict[str, List[str]]:
        """Obtient la liste des contextes existants par domaine"""
        existing_contexts = {}
        
        for domain_dir in self.context_dir.iterdir():
            if domain_dir.is_dir():
                domain = domain_dir.name
                existing_contexts[domain] = []
                
                for context_file in domain_dir.glob("*.txt"):
                    # Extraire le nom de la bibliothÃ¨que du nom de fichier
                    filename = context_file.stem
                    
                    # GÃ©rer les diffÃ©rents formats de noms de fichiers
                    if filename.endswith("-context"):
                        lib_name = filename[:-9]  # Supprimer "-context"
                    elif filename.endswith("_context"):
                        lib_name = filename[:-9]  # Supprimer "_context"
                    else:
                        # Pour les fichiers sans suffixe context (comme numcosmo.txt)
                        lib_name = filename
                    
                    existing_contexts[domain].append(lib_name)
        
        return existing_contexts
    
    def load_libraries_data(self) -> Dict[str, List[Dict]]:
        """Charge les donnÃ©es des bibliothÃ¨ques depuis les fichiers JSON"""
        libraries = {}
        
        for domain in self.domains:
            json_file = self.data_dir / f"{domain}-libraries.json"
            if json_file.exists():
                with open(json_file, 'r', encoding='utf-8') as f:
                    data = json.load(f)
                    libraries[domain] = data.get('libraries', [])
            else:
                libraries[domain] = []
        
        return libraries
    
    def extract_repo_info(self, github_url: str) -> Optional[Dict]:
        """Extrait les informations du repository depuis l'URL GitHub"""
        try:
            # Nettoyer l'URL
            url = github_url.strip()
            if not url.startswith('http'):
                url = f"https://{url}"
            
            # Extraire owner et repo
            if "github.com" in url:
                parts = url.split("github.com/")[1].split("/")
                if len(parts) >= 2:
                    owner = parts[0]
                    repo = parts[1].replace(".git", "")
                    return {"owner": owner, "repo": repo}
            
            return None
            
        except Exception as e:
            self.logger.error(f"Error extracting repo info from {github_url}: {e}")
            return None
    
    def generate_missing_contexts(self):
        """GÃ©nÃ¨re les contextes manquants pour toutes les bibliothÃ¨ques"""
        self.logger.info("ğŸš€ GÃ©nÃ©ration des contextes manquants...")
        
        # VÃ©rifier que contextmaker et git sont disponibles
        if not self.check_contextmaker_available():
            self.logger.error("âŒ Cannot continue without contextmaker")
            return
        
        if not self.check_git_available():
            self.logger.error("âŒ Cannot continue without git")
            return
        
        # Charger les donnÃ©es existantes
        existing_contexts = self.get_existing_contexts()
        libraries_data = self.load_libraries_data()
        
        self.logger.info(f"Existing contexts: {existing_contexts}")
        self.logger.info(f"Found libraries: {list(libraries_data.keys())}")
        
        total_generated = 0
        
        for domain, libraries in libraries_data.items():
            self.logger.info(f"Processing domain: {domain}")
            
            existing_libs = existing_contexts.get(domain, [])
            
            for lib in libraries:
                lib_name = lib.get('name', '').replace('/', '-').replace('_', '-')
                github_url = lib.get('github_url', '')
                
                if not github_url:
                    self.logger.warning(f"No GitHub URL for {lib_name}")
                    continue
                
                # VÃ©rifier si le contexte existe dÃ©jÃ  (comparaison plus robuste)
                context_exists = False
                
                # Cas spÃ©ciaux de correspondance
                special_mappings = {
                    "mperrin-poppy": "spacetelescope-popp",
                    "trasal-frbpoppy": "trasal-frbpopp", 
                    "rbvi-ChimeraX": "rbvi-Chimera",
                    "schrodinger-pymol-open-source": "schrodinger-pymol-open-sourc",
                }
                
                # VÃ©rifier d'abord les correspondances spÃ©ciales
                if lib_name in special_mappings:
                    expected_match = special_mappings[lib_name]
                    for existing_lib in existing_libs:
                        if expected_match in existing_lib:
                            context_exists = True
                            self.logger.info(f"Existing context for {lib_name} (matches {existing_lib}), skipped")
                            break
                
                # Si pas de correspondance spÃ©ciale, utiliser la logique gÃ©nÃ©rique
                if not context_exists:
                    for existing_lib in existing_libs:
                        # VÃ©rifier si le nom de la bibliothÃ¨que (sans namespace) correspond
                        if existing_lib in lib_name or lib_name.endswith(existing_lib):
                            context_exists = True
                            self.logger.info(f"Existing context for {lib_name} (matches {existing_lib}), skipped")
                            break
                
                if context_exists:
                    continue
                
                # Extraire les informations du repository
                repo_info = self.extract_repo_info(github_url)
                if not repo_info:
                    self.logger.error(f"Unable to extract repo info: {github_url}")
                    continue
                
                package_name = repo_info['repo']
                
                # 1. Cloner le repository
                if not self.clone_repository(repo_info['owner'], repo_info['repo'], package_name):
                    self.logger.error(f"Failed to clone {package_name}")
                    continue
                
                # 2. GÃ©nÃ©rer le contenu du contexte avec contextmaker
                content = self.generate_context_with_contextmaker(package_name, lib_name, domain)
                
                if content:
                    # Sauvegarder le fichier
                    if self.save_context_file(domain, lib_name, content):
                        total_generated += 1
                    
                    # Pause pour Ã©viter la surcharge
                    time.sleep(2)
                else:
                    self.logger.warning(f"Unable to generate context for {lib_name}")
        
        self.logger.info(f"Generation complete. {total_generated} new contexts created.")

    # ============================================================================
    # 3. NETTOYAGE DES DOUBLONS
    # ============================================================================
    
    def cleanup_duplicate_contexts_legacy(self):
        """Nettoyage legacy: Supprime les fichiers de contexte de public/context racine qui sont dans les sous-rÃ©pertoires de domaines"""
        self.logger.info("ğŸ§¹ Nettoyage legacy des doublons racine...")
        
        if not self.context_dir.exists():
            self.logger.info("public/context directory does not exist")
            return
        
        # Obtenir tous les fichiers de contexte dans public/context (niveau racine)
        root_context_files = []
        for file in self.context_dir.iterdir():
            if file.is_file() and file.name.endswith('-context.txt'):
                root_context_files.append(file.name)
        
        if not root_context_files:
            self.logger.info("No context files found in public/context root")
            return
        
        self.logger.info(f"Found {len(root_context_files)} context files in public/context root")
        
        # VÃ©rifier chaque sous-rÃ©pertoire de domaine
        files_to_remove = []
        
        for domain in self.domains:
            domain_dir = self.context_dir / domain
            if not domain_dir.exists():
                continue
                
            self.logger.info(f"Checking domain: {domain}")
            
            # Obtenir les fichiers de contexte dans le rÃ©pertoire de domaine
            domain_files = []
            for file in domain_dir.iterdir():
                if file.is_file() and file.name.endswith('.txt'):
                    domain_files.append(file.name)
            
            self.logger.info(f"  Found {len(domain_files)} context files in {domain}/")
            
            # Trouver les doublons
            for root_file in root_context_files:
                if root_file in domain_files:
                    files_to_remove.append(self.context_dir / root_file)
                    self.logger.info(f"  Found duplicate: {root_file}")
        
        # Supprimer les fichiers dupliquÃ©s
        if files_to_remove:
            self.logger.info(f"Removing {len(files_to_remove)} duplicate files...")
            
            for file_path in files_to_remove:
                try:
                    file_path.unlink()
                    self.logger.info(f"  Removed: {file_path.name}")
                except Exception as e:
                    self.logger.error(f"  Error removing {file_path.name}: {e}")
            
            self.logger.info("Legacy cleanup completed successfully")
        else:
            self.logger.info("No duplicate files found to remove")
    
    def cleanup_all_duplicates(self):
        """Nettoyage avancÃ©: Garde le contexte le plus long pour chaque bibliothÃ¨que"""
        self.logger.info("ğŸ§¹ Nettoyage avancÃ© des doublons...")
        
        for domain in self.domains:
            domain_dir = self.context_dir / domain
            if not domain_dir.exists():
                continue
            
            self.logger.info(f"Processing domain: {domain}")
            
            # Grouper les fichiers par bibliothÃ¨que
            library_files = {}
            
            for context_file in domain_dir.glob("*.txt"):
                if not context_file.name.endswith('.txt'):
                    continue
                
                # Extraire le nom de la bibliothÃ¨que
                lib_name = self.extract_library_name_from_filename(context_file.name)
                
                if lib_name not in library_files:
                    library_files[lib_name] = []
                
                library_files[lib_name].append(context_file)
            
            # Pour chaque bibliothÃ¨que, garder le fichier le plus long
            for lib_name, files in library_files.items():
                if len(files) > 1:
                    self.logger.info(f"  Found {len(files)} files for {lib_name}")
                    
                    # Trier par taille (le plus long en premier)
                    files.sort(key=lambda f: f.stat().st_size, reverse=True)
                    
                    # Garder le premier (le plus long) et supprimer les autres
                    for file_to_remove in files[1:]:
                        try:
                            file_to_remove.unlink()
                            self.logger.info(f"    Removed: {file_to_remove.name}")
                        except Exception as e:
                            self.logger.error(f"    Error removing {file_to_remove.name}: {e}")
    
    def extract_library_name_from_filename(self, filename: str) -> str:
        """Extrait le nom de la bibliothÃ¨que du nom de fichier"""
        # Supprimer les suffixes communs
        name = filename
        if name.endswith('-context.txt'):
            name = name[:-14]
        elif name.endswith('.txt'):
            name = name[:-4]
        
        # GÃ©rer les diffÃ©rents patterns de nommage
        if '-' in name:
            parts = name.split('-')
            
            # Chercher la partie la plus descriptive
            if len(parts) >= 2:
                # DerniÃ¨re partie pourrait Ãªtre le nom de la bibliothÃ¨que
                potential_names = []
                potential_names.append(parts[-1])
                
                # Avant-derniÃ¨re pourrait Ãªtre le nom de la bibliothÃ¨que
                if len(parts) >= 3:
                    potential_names.append(parts[-2])
                
                # Cas spÃ©cial: "lesgourg-class-public" -> "class_public"
                if len(parts) >= 3 and parts[0] == 'lesgourg' and parts[1] == 'class':
                    return 'class_public'
                
                # Chercher la partie la plus descriptive
                for part in potential_names:
                    if len(part) > 2 and not part.startswith('astro') and not part.startswith('cosmo'):
                        return part
                
                # Fallback: retourner la derniÃ¨re partie
                return parts[-1]
        
        return name
    
    def cleanup_duplicate_contexts(self):
        """Nettoie tous les contextes dupliquÃ©s"""
        self.logger.info("ğŸ§¹ DÃ©but du nettoyage complet des contextes dupliquÃ©s...")
        
        # D'abord le nettoyage legacy (doublons racine)
        self.cleanup_duplicate_contexts_legacy()
        
        # Puis le nettoyage avancÃ© (garder le plus long pour chaque bibliothÃ¨que)
        self.cleanup_all_duplicates()
        
        self.logger.info("âœ… Nettoyage complet des contextes dupliquÃ©s terminÃ©")

    # ============================================================================
    # 4. MISE Ã€ JOUR DES STATUTS JSON
    # ============================================================================
    
    def update_library_metadata(self):
        """Met Ã  jour les mÃ©tadonnÃ©es des bibliothÃ¨ques pour tous les domaines"""
        self.logger.info("ğŸ“ Mise Ã  jour des mÃ©tadonnÃ©es des bibliothÃ¨ques...")
        
        try:
            # Scanner les fichiers de contexte
            context_files = self.scan_context_files()
            
            # Mettre Ã  jour tous les domaines
            for domain in self.domains:
                self.logger.info(f"  Mise Ã  jour du domaine: {domain}")
                
                # Charger les donnÃ©es du domaine
                domain_data = self.load_domain_data(domain)
                
                # Mettre Ã  jour les bibliothÃ¨ques du domaine
                for lib in domain_data["libraries"]:
                    lib_name = lib["name"]
                    
                    # Chercher le fichier de contexte correspondant
                    has_context = False
                    context_filename = None
                    
                    # CrÃ©er une liste des noms de fichiers de contexte disponibles
                    available_context_files = [file_info["filename"] for file_info in context_files.get(domain, [])]
                    
                    # Cas spÃ©ciaux de correspondance
                    special_mappings = {
                        "mperrin/poppy": "spacetelescope-poppy-context.txt",
                        "trasal/frbpoppy": "trasal-frbpoppy-context.txt",
                        "rbvi/ChimeraX": "rbvi-ChimeraX-context.txt",
                        "schrodinger/pymol-open-source": "schrodinger-pymol-open-source-context.txt",
                    }
                    
                    # VÃ©rifier d'abord les correspondances spÃ©ciales
                    if lib_name in special_mappings:
                        expected_file = special_mappings[lib_name]
                        if expected_file in available_context_files:
                            has_context = True
                            context_filename = expected_file
                    
                    # Si pas de correspondance spÃ©ciale, essayer les patterns gÃ©nÃ©riques
                    if not has_context:
                        patterns = [
                            # Pattern exact avec tirets
                            f"{lib_name.replace('/', '-').replace('_', '-').replace('.', '-')}-context.txt",
                            # Pattern avec seulement le nom du repo
                            f"{lib_name.split('/')[-1]}-context.txt",
                            # Pattern avec remplacement des underscores
                            f"{lib_name.replace('/', '-').replace('_', '-')}-context.txt",
                            # Pattern avec remplacement des points
                            f"{lib_name.replace('/', '-').replace('.', '-')}-context.txt",
                            # Pattern simplifiÃ© (juste le nom du repo)
                            f"{lib_name.split('/')[-1].replace('_', '-')}-context.txt",
                            # Pattern avec tirets pour tous les sÃ©parateurs
                            f"{lib_name.replace('/', '-').replace('_', '-').replace('.', '-')}-context.txt",
                        ]
                        
                        # Chercher le fichier correspondant
                        for pattern in patterns:
                            if pattern in available_context_files:
                                has_context = True
                                context_filename = pattern
                                break
                    
                    # Si aucun pattern ne correspond, chercher par similaritÃ©
                    if not has_context:
                        lib_repo = lib_name.split('/')[-1].lower()
                        for filename in available_context_files:
                            context_name = filename.replace('-context.txt', '').lower()
                            if lib_repo in context_name or context_name in lib_repo:
                                has_context = True
                                context_filename = filename
                                break
                    
                    lib["hasContextFile"] = has_context
                    if has_context:
                        lib["contextFileName"] = context_filename
                    else:
                        # Supprimer la propriÃ©tÃ© si elle existe
                        if "contextFileName" in lib:
                            del lib["contextFileName"]
                
                # Sauvegarder le domaine
                domain_file = self.data_dir / f"{domain}-libraries.json"
                with open(domain_file, 'w') as f:
                    json.dump(domain_data, f, indent=2)
                
                self.logger.info(f"  âœ… {domain}: {len(domain_data['libraries'])} bibliothÃ¨ques mises Ã  jour")
            
            self.logger.info("âœ… MÃ©tadonnÃ©es des bibliothÃ¨ques mises Ã  jour pour tous les domaines")
            
        except Exception as e:
            self.logger.error(f"âŒ Erreur mise Ã  jour mÃ©tadonnÃ©es: {e}")
    
    def scan_context_files(self) -> Dict[str, List[Dict]]:
        """Scanne les fichiers de contexte"""
        context_files = {}
        
        for domain_dir in self.context_dir.iterdir():
            if domain_dir.is_dir():
                domain = domain_dir.name
                context_files[domain] = []
                
                for context_file in domain_dir.glob("*.txt"):
                    file_hash = self.get_file_hash(context_file)
                    context_files[domain].append({
                        "filename": context_file.name,
                        "path": str(context_file),
                        "hash": file_hash,
                        "size": context_file.stat().st_size
                    })
        
        return context_files
    
    def load_domain_data(self, domain: str) -> Dict:
        """Charge les donnÃ©es d'un domaine"""
        domain_json = self.data_dir / f"{domain}-libraries.json"
        if domain_json.exists():
            with open(domain_json, 'r') as f:
                return json.load(f)
        return {"libraries": [], "domain": domain}
    
    def load_astronomy_data(self) -> Dict:
        """Charge les donnÃ©es astronomy"""
        return self.load_domain_data("astronomy")
    
    def load_finance_data(self) -> Dict:
        """Charge les donnÃ©es finance"""
        return self.load_domain_data("finance")

    # ============================================================================
    # 5. GÃ‰NÃ‰RATION DU MODULE EMBEDDED-CONTEXT.TS
    # ============================================================================
    
    def generate_embedded_context(self):
        """GÃ©nÃ¨re le module embedded-context.ts"""
        try:
            # Charger la configuration
            config = json.load(open(self.config_file, 'r'))
            programs = config.get("programs", [])
            
            # GÃ©nÃ©rer le contenu embarquÃ©
            combined_contexts = {}
            
            for program in programs:
                program_id = program.get("id")
                combined_context_file = program.get("combinedContextFile")
                
                if not program_id:
                    continue
                
                if combined_context_file and combined_context_file.startswith("/api/context/"):
                    # C'est un fichier de contexte dynamique
                    combined_contexts[program_id] = f"Context loaded from: {combined_context_file}"
                else:
                    # Pas de contexte dÃ©fini
                    combined_contexts[program_id] = f"No context files defined for {program_id}"
            
            # GÃ©nÃ©rer le contenu TypeScript
            context_lines = []
            for id, content in combined_contexts.items():
                context_lines.append(f"  '{id}': {json.dumps(content)},")
            
            ts_content = f"""// This file is auto-generated. Do not edit directly.
// Generated on {datetime.now().isoformat()}

/**
 * Embedded context content for each program
 * This avoids having to load context files at runtime
 */
export const embeddedContexts: Record<string, string> = {{
{chr(10).join(context_lines)}
}};

/**
 * Get the embedded context for a program
 * @param programId The ID of the program
 * @returns The embedded context content or undefined if not found
 */
export function getEmbeddedContext(programId: string): string | undefined {{
  return embeddedContexts[programId];
}}
"""
            
            # Ã‰crire le fichier
            output_path = self.base_dir / "app" / "utils" / "embedded-context.ts"
            output_path.parent.mkdir(parents=True, exist_ok=True)
            
            with open(output_path, 'w', encoding='utf-8') as f:
                f.write(ts_content)
            
            self.logger.info(f"âœ… Module embedded-context.ts gÃ©nÃ©rÃ©: {output_path}")
            
        except Exception as e:
            self.logger.error(f"âŒ Erreur gÃ©nÃ©ration embedded context: {e}")

    # ============================================================================
    # 6. RÃ‰GÃ‰NÃ‰RATION DE CONFIG.JSON
    # ============================================================================
    
    def regenerate_config(self):
        """RÃ©gÃ©nÃ¨re config.json"""
        try:
            script_path = self.base_dir / "scripts" / "core" / "generate-programs-from-libraries.py"
            
            if script_path.exists():
                result = subprocess.run(
                    ["python3", str(script_path)],
                    capture_output=True,
                    text=True,
                    cwd=self.base_dir
                )
                
                if result.returncode == 0:
                    self.logger.info("âœ… config.json rÃ©gÃ©nÃ©rÃ© avec succÃ¨s")
                    if result.stdout:
                        self.logger.info(f"Output: {result.stdout}")
                else:
                    self.logger.error(f"âŒ Erreur rÃ©gÃ©nÃ©ration config: {result.stderr}")
            else:
                self.logger.error("âŒ Script generate-programs-from-libraries.py introuvable")
                
        except Exception as e:
            self.logger.error(f"âŒ Erreur exÃ©cution script: {e}")

    # ============================================================================
    # 7. GESTION DES HASHES ET DÃ‰TECTION DES CHANGEMENTS
    # ============================================================================
    
    def update_context_hashes(self):
        """Met Ã  jour les hashes des contextes"""
        context_files = self.scan_context_files()
        
        for domain, files in context_files.items():
            for file_info in files:
                filename = file_info["filename"]
                file_hash = file_info["hash"]
                
                if domain not in self.state["context_hashes"]:
                    self.state["context_hashes"][domain] = {}
                
                self.state["context_hashes"][domain][filename] = file_hash
    
    def has_changes(self) -> bool:
        """VÃ©rifie s'il y a des changements dans les fichiers de contexte"""
        current_files = self.scan_context_files()
        
        for domain, files in current_files.items():
            if domain not in self.state["context_hashes"]:
                return True
            
            for file_info in files:
                filename = file_info["filename"]
                current_hash = file_info["hash"]
                
                if filename not in self.state["context_hashes"][domain]:
                    return True
                
                if self.state["context_hashes"][domain][filename] != current_hash:
                    return True
        
        return False

    # ============================================================================
    # 8. EXÃ‰CUTION PRINCIPALE UNIFIÃ‰E
    # ============================================================================
    
    def run_full_update(self, force: bool = False):
        """ExÃ©cute la mise Ã  jour complÃ¨te unifiÃ©e"""
        self.logger.info("ğŸš€ DÃ‰BUT DE LA MISE Ã€ JOUR UNIFIÃ‰E DES CONTEXTES")
        self.logger.info("=" * 60)
        
        try:
            # 1. VÃ©rification de la structure
            self.logger.info("\nğŸ“‹ Ã‰TAPE 1: VÃ©rification de la structure des contextes")
            if not self.verify_context_structure():
                self.logger.warning("âš ï¸  ProblÃ¨mes de structure dÃ©tectÃ©s, mais continuation...")
            
            # 2. GÃ©nÃ©ration des contextes manquants
            self.logger.info("\nğŸ”„ Ã‰TAPE 2: GÃ©nÃ©ration des contextes manquants")
            self.generate_missing_contexts()
            
            # 3. Nettoyage des doublons
            self.logger.info("\nğŸ§¹ Ã‰TAPE 3: Nettoyage des contextes dupliquÃ©s")
            self.cleanup_duplicate_contexts()
            
            # 4. Mise Ã  jour des mÃ©tadonnÃ©es
            self.logger.info("\nğŸ“ Ã‰TAPE 4: Mise Ã  jour des mÃ©tadonnÃ©es")
            self.update_library_metadata()
            
            # 5. RÃ©gÃ©nÃ©ration de config.json
            self.logger.info("\nâš™ï¸  Ã‰TAPE 5: RÃ©gÃ©nÃ©ration de config.json")
            self.regenerate_config()
            
            # 6. GÃ©nÃ©ration du module embedded
            self.logger.info("\nğŸ“¦ Ã‰TAPE 6: GÃ©nÃ©ration du module embedded-context.ts")
            self.generate_embedded_context()
            
            # 7. Mise Ã  jour des hashes et de l'Ã©tat
            self.logger.info("\nğŸ’¾ Ã‰TAPE 7: Mise Ã  jour de l'Ã©tat")
            self.update_context_hashes()
            self.state["last_update"] = datetime.now().isoformat()
            self.state["update_count"] += 1
            self.save_state()
            
            # 8. Nettoyage des repositories temporaires
            self.logger.info("\nğŸ—‘ï¸  Ã‰TAPE 8: Nettoyage des repositories temporaires")
            if self.temp_dir.exists():
                shutil.rmtree(self.temp_dir)
                self.temp_dir.mkdir(parents=True, exist_ok=True)
            
            self.logger.info("\nğŸ‰ MISE Ã€ JOUR UNIFIÃ‰E TERMINÃ‰E AVEC SUCCÃˆS!")
            
        except Exception as e:
            self.logger.error(f"âŒ Erreur lors de la mise Ã  jour unifiÃ©e: {e}")
            raise
        finally:
            # Mise Ã  jour de la derniÃ¨re vÃ©rification
            self.state["last_check"] = datetime.now().isoformat()
            self.save_state()
    
    def run_quick_update(self):
        """ExÃ©cute une mise Ã  jour rapide avec dÃ©tection des modifications GitHub"""
        self.logger.info("ğŸš€ DÃ‰BUT DE LA MISE Ã€ JOUR RAPIDE DES CONTEXTES")
        self.logger.info("=" * 50)
        
        try:
            # 1. VÃ©rification de la structure
            self.logger.info("\nğŸ“‹ Ã‰TAPE 1: VÃ©rification de la structure des contextes")
            if not self.verify_context_structure():
                self.logger.warning("âš ï¸  ProblÃ¨mes de structure dÃ©tectÃ©s, mais continuation...")
            
            # 2. DÃ©tection des modifications GitHub et marquage des contextes obsolÃ¨tes
            # self.logger.info("\nğŸ” Ã‰TAPE 2: DÃ©tection des modifications des repositories GitHub")
            # self.mark_outdated_contexts_as_missing()  # CommentÃ© temporairement
            
            # 3. Nettoyage des doublons
            self.logger.info("\nğŸ§¹ Ã‰TAPE 3: Nettoyage des contextes dupliquÃ©s")
            self.cleanup_duplicate_contexts()
            
            # 4. Mise Ã  jour des mÃ©tadonnÃ©es
            self.logger.info("\nğŸ“ Ã‰TAPE 4: Mise Ã  jour des mÃ©tadonnÃ©es")
            self.update_library_metadata()
            
            # 5. RÃ©gÃ©nÃ©ration de config.json
            self.logger.info("\nâš™ï¸  Ã‰TAPE 5: RÃ©gÃ©nÃ©ration de config.json")
            self.regenerate_config()
            
            # 6. GÃ©nÃ©ration du module embedded
            self.logger.info("\nğŸ“¦ Ã‰TAPE 6: GÃ©nÃ©ration du module embedded-context.ts")
            self.generate_embedded_context()
            
            # 7. Mise Ã  jour des hashes et de l'Ã©tat
            self.logger.info("\nğŸ’¾ Ã‰TAPE 7: Mise Ã  jour de l'Ã©tat")
            self.update_context_hashes()
            self.state["last_update"] = datetime.now().isoformat()
            self.state["update_count"] += 1
            self.save_state()
            
            self.logger.info("\nğŸ‰ MISE Ã€ JOUR RAPIDE TERMINÃ‰E AVEC SUCCÃˆS!")
            
        except Exception as e:
            self.logger.error(f"âŒ Erreur lors de la mise Ã  jour rapide: {e}")
            raise
        finally:
            # Mise Ã  jour de la derniÃ¨re vÃ©rification
            self.state["last_check"] = datetime.now().isoformat()
            self.save_state()
    
    def cleanup_pycache(self):
        """Nettoie tous les dossiers __pycache__ et fichiers .pyc"""
        self.logger.info("ğŸ§¹ DÃ‰BUT DU NETTOYAGE DES CACHES PYTHON")
        self.logger.info("=" * 50)
        
        try:
            # Trouver les Ã©lÃ©ments Ã  nettoyer
            self.logger.info("ğŸ” Recherche des dossiers et fichiers Ã  nettoyer...")
            pycache_dirs = self.find_pycache_dirs()
            pyc_files = self.find_pyc_files()
            
            self.logger.info(f"ğŸ“ Dossiers __pycache__ trouvÃ©s: {len(pycache_dirs)}")
            self.logger.info(f"ğŸ“„ Fichiers .pyc trouvÃ©s: {len(pyc_files)}")
            
            if not pycache_dirs and not pyc_files:
                self.logger.info("âœ¨ Aucun cache Ã  nettoyer - projet dÃ©jÃ  propre!")
                return {"pycache_dirs": 0, "pyc_files": 0, "total_cleaned": 0}
            
            # Nettoyer les dossiers __pycache__
            self.logger.info("\nğŸ—‘ï¸  Suppression des dossiers __pycache__...")
            cleaned_dirs = self.cleanup_pycache_dirs(pycache_dirs)
            
            # Nettoyer les fichiers .pyc
            self.logger.info("\nğŸ—‘ï¸  Suppression des fichiers .pyc...")
            cleaned_files = self.cleanup_pyc_files(pyc_files)
            
            # RÃ©sumÃ©
            total_cleaned = cleaned_dirs + cleaned_files
            self.logger.info(f"\nğŸ“Š RÃ‰SUMÃ‰ DU NETTOYAGE:")
            self.logger.info("=" * 30)
            self.logger.info(f"âœ… Dossiers __pycache__ supprimÃ©s: {cleaned_dirs}")
            self.logger.info(f"âœ… Fichiers .pyc supprimÃ©s: {cleaned_files}")
            self.logger.info(f"âœ… Total des Ã©lÃ©ments supprimÃ©s: {total_cleaned}")
            
            return {
                "pycache_dirs": cleaned_dirs,
                "pyc_files": cleaned_files,
                "total_cleaned": total_cleaned
            }
            
        except Exception as e:
            self.logger.error(f"âŒ Erreur lors du nettoyage des caches: {e}")
            raise
    
    def find_pycache_dirs(self) -> list:
        """Trouve tous les dossiers __pycache__"""
        pycache_dirs = []
        
        for root, dirs, files in os.walk(self.base_dir):
            if '__pycache__' in dirs:
                pycache_path = Path(root) / '__pycache__'
                pycache_dirs.append(pycache_path)
        
        return pycache_dirs
    
    def find_pyc_files(self) -> list:
        """Trouve tous les fichiers .pyc"""
        pyc_files = []
        
        for root, dirs, files in os.walk(self.base_dir):
            for file in files:
                if file.endswith('.pyc'):
                    pyc_path = Path(root) / file
                    pyc_files.append(pyc_path)
        
        return pyc_files
    
    def cleanup_pycache_dirs(self, pycache_dirs: list) -> int:
        """Nettoie les dossiers __pycache__"""
        cleaned_count = 0
        
        for pycache_dir in pycache_dirs:
            try:
                if pycache_dir.exists():
                    # Calculer la taille avant suppression
                    total_size = sum(f.stat().st_size for f in pycache_dir.rglob('*') if f.is_file())
                    size_mb = total_size / (1024 * 1024)
                    
                    # Supprimer le dossier
                    shutil.rmtree(pycache_dir)
                    self.logger.info(f"ğŸ—‘ï¸  SupprimÃ©: {pycache_dir} ({size_mb:.2f} MB)")
                    cleaned_count += 1
                    
            except Exception as e:
                self.logger.error(f"âŒ Erreur suppression {pycache_dir}: {e}")
        
        return cleaned_count
    
    def cleanup_pyc_files(self, pyc_files: list) -> int:
        """Nettoie les fichiers .pyc"""
        cleaned_count = 0
        
        for pyc_file in pyc_files:
            try:
                if pyc_file.exists():
                    # Calculer la taille avant suppression
                    size_bytes = pyc_file.stat().st_size
                    size_kb = size_bytes / 1024
                    
                    # Supprimer le fichier
                    pyc_file.unlink()
                    self.logger.info(f"ğŸ—‘ï¸  SupprimÃ©: {pyc_file} ({size_kb:.2f} KB)")
                    cleaned_count += 1
                    
            except Exception as e:
                self.logger.error(f"âŒ Erreur suppression {pyc_file}: {e}")
        
        return cleaned_count
    
    def cleanup_log_files(self):
        """Nettoie les anciens fichiers de logs"""
        self.logger.info("ğŸ§¹ Nettoyage des fichiers de logs...")
        
        try:
            # Logs Ã  nettoyer (anciens scripts supprimÃ©s)
            old_logs = [
                "cleanup_duplicates.log",
                "cleanup_pycache.log", 
                "context_generation.log",
                "context_manager.log",
                "json_status_update.log",
                "unified_context_manager.log"
            ]
            
            # Logs Ã  conserver (scripts actifs)
            active_logs = [
                "maintenance.log",
                "scheduler.log"
            ]
            
            cleaned_count = 0
            total_size_mb = 0
            
            for log_file in old_logs:
                log_path = self.base_dir / log_file
                if log_path.exists():
                    try:
                        # Calculer la taille avant suppression
                        size_bytes = log_path.stat().st_size
                        size_mb = size_bytes / (1024 * 1024)
                        total_size_mb += size_mb
                        
                        # Supprimer le fichier
                        log_path.unlink()
                        self.logger.info(f"ğŸ—‘ï¸  SupprimÃ©: {log_file} ({size_mb:.2f} MB)")
                        cleaned_count += 1
                        
                    except Exception as e:
                        self.logger.error(f"âŒ Erreur suppression {log_file}: {e}")
            
            # Nettoyer aussi les logs du dossier logs/ (plus de 7 jours)
            logs_dir = self.base_dir / "logs"
            if logs_dir.exists():
                cutoff_date = datetime.now() - timedelta(days=7)
                for log_file in logs_dir.glob("*.log"):
                    if log_file.stat().st_mtime < cutoff_date.timestamp():
                        try:
                            size_bytes = log_file.stat().st_size
                            size_mb = size_bytes / (1024 * 1024)
                            total_size_mb += size_mb
                            
                            log_file.unlink()
                            self.logger.info(f"ğŸ—‘ï¸  SupprimÃ©: logs/{log_file.name} ({size_mb:.2f} MB)")
                            cleaned_count += 1
                            
                        except Exception as e:
                            self.logger.error(f"âŒ Erreur suppression logs/{log_file.name}: {e}")
            
            if cleaned_count > 0:
                self.logger.info(f"âœ… Nettoyage des logs terminÃ©: {cleaned_count} fichiers supprimÃ©s ({total_size_mb:.2f} MB)")
            else:
                self.logger.info("âœ¨ Aucun log Ã  nettoyer")
                
        except Exception as e:
            self.logger.error(f"âŒ Erreur lors du nettoyage des logs: {e}")
    
    def cleanup_all(self):
        """Nettoie tout : caches Python, logs et contextes dupliquÃ©s"""
        self.logger.info("ğŸ§¹ DÃ‰BUT DU NETTOYAGE COMPLET")
        self.logger.info("=" * 50)
        
        try:
            # 1. Nettoyage des caches Python
            self.logger.info("\nğŸ—‘ï¸  Ã‰TAPE 1: Nettoyage des caches Python")
            pycache_result = self.cleanup_pycache()
            
            # 2. Nettoyage des logs
            self.logger.info("\nğŸ—‘ï¸  Ã‰TAPE 2: Nettoyage des logs")
            self.cleanup_log_files()
            
            # 3. Nettoyage des contextes dupliquÃ©s
            self.logger.info("\nğŸ—‘ï¸  Ã‰TAPE 3: Nettoyage des contextes dupliquÃ©s")
            self.cleanup_duplicate_contexts()
            
            self.logger.info("\nğŸ‰ NETTOYAGE COMPLET TERMINÃ‰!")
            
        except Exception as e:
            self.logger.error(f"âŒ Erreur lors du nettoyage complet: {e}")
            raise

def main():
    """Point d'entrÃ©e principal"""
    import argparse
    
    parser = argparse.ArgumentParser(
        description="Gestionnaire unifiÃ© des contextes - Remplace tous les scripts de maintenance des contextes",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
Exemples d'utilisation:
  python3 context-manager-unified.py --full           # Mise Ã  jour complÃ¨te avec gÃ©nÃ©ration de contextes
  python3 context-manager-unified.py --quick          # Mise Ã  jour rapide sans gÃ©nÃ©ration
  python3 context-manager-unified.py --verify         # VÃ©rification de la structure uniquement
  python3 context-manager-unified.py --cleanup        # Nettoyage des doublons uniquement
  python3 context-manager-unified.py --cleanup-pycache # Nettoyage des caches Python uniquement
  python3 context-manager-unified.py --cleanup-all    # Nettoyage complet (caches, logs et doublons)
        """
    )
    
    parser.add_argument("--full", action="store_true", help="Mise Ã  jour complÃ¨te avec gÃ©nÃ©ration de contextes")
    parser.add_argument("--quick", action="store_true", help="Mise Ã  jour rapide sans gÃ©nÃ©ration")
    parser.add_argument("--verify", action="store_true", help="VÃ©rification de la structure uniquement")
    parser.add_argument("--cleanup", action="store_true", help="Nettoyage des doublons uniquement")
    parser.add_argument("--cleanup-pycache", action="store_true", help="Nettoyage des caches Python uniquement")
    parser.add_argument("--cleanup-all", action="store_true", help="Nettoyage complet (caches, logs et doublons)")
    parser.add_argument("--base-dir", default=".", help="RÃ©pertoire de base")
    
    args = parser.parse_args()
    
    manager = UnifiedContextManager(args.base_dir)
    
    if args.verify:
        manager.verify_context_structure()
    elif args.cleanup:
        manager.cleanup_duplicate_contexts()
    elif args.cleanup_pycache:
        manager.cleanup_pycache()
    elif args.cleanup_all:
        manager.cleanup_all()
    elif args.quick:
        manager.run_quick_update()
    elif args.full:
        manager.run_full_update()
    else:
        # Par dÃ©faut, exÃ©cuter la mise Ã  jour rapide
        manager.run_quick_update()

if __name__ == "__main__":
    main()
