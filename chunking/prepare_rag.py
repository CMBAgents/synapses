#!/usr/bin/env python3
"""
Script pour prÃ©parer les contextes pour le RAG (VERSION OPTIMISÃ‰E)
- DÃ©coupe les contextes en chunks
- CrÃ©e les embeddings
- Indexe dans ChromaDB avec optimisations pour rÃ©duire la taille
"""

import os
import json
import sqlite3
from pathlib import Path
from typing import List, Dict
import chromadb
from chromadb.config import Settings
from sentence_transformers import SentenceTransformer
from langchain.text_splitter import RecursiveCharacterTextSplitter
from tqdm import tqdm

# Configuration
CONTEXT_DIR = Path("public/context")
CHROMA_DIR = Path("chroma_db")
CHUNK_SIZE = 1000  # CaractÃ¨res par chunk
CHUNK_OVERLAP = 200  # Overlap pour maintenir le contexte
MODEL_NAME = "all-MiniLM-L6-v2"  # ModÃ¨le gratuit, rapide, performant

print(" Initialisation du RAG Chunking System")
print("=" * 60)

# 1. Initialiser le modÃ¨le d'embedding
print("\n Chargement du modÃ¨le d'embedding...")
embedding_model = SentenceTransformer(MODEL_NAME)
print(f" ModÃ¨le chargÃ©: {MODEL_NAME}")

# 2. Initialiser ChromaDB avec configuration optimisÃ©e
print("\nğŸ”§ Initialisation de ChromaDB (mode optimisÃ©)...")

# Configuration pour minimiser la taille de la DB
chroma_settings = Settings(
    anonymized_telemetry=False,
    allow_reset=True,
)

chroma_client = chromadb.PersistentClient(
    path=str(CHROMA_DIR),
    settings=chroma_settings
)

# CrÃ©er ou rÃ©cupÃ©rer la collection
try:
    # Supprimer l'ancienne collection si elle existe
    try:
        chroma_client.delete_collection(name="library_contexts")
        print("  âœ“ Ancienne collection supprimÃ©e")
    except:
        pass
    
    # CrÃ©er la collection SANS full-text search
    collection = chroma_client.create_collection(
        name="library_contexts",
        metadata={
            "description": "Library documentation chunks",
            "hnsw:space": "cosine"  # Utiliser cosine similarity
        }
    )
    print("âœ… Collection crÃ©Ã©e: library_contexts (optimisÃ©e)")
except Exception as e:
    print(f"âŒ Erreur lors de la crÃ©ation de la collection: {e}")
    collection = chroma_client.get_collection(name="library_contexts")

# 3. Initialiser le text splitter
text_splitter = RecursiveCharacterTextSplitter(
    chunk_size=CHUNK_SIZE,
    chunk_overlap=CHUNK_OVERLAP,
    length_function=len,
    separators=["\n\n", "\n", ". ", " ", ""]
)

# 4. Traiter tous les fichiers de contexte
def process_context_file(file_path: Path, domain: str) -> int:
    """
    DÃ©coupe un fichier de contexte en chunks et l'indexe
    Retourne le nombre de chunks crÃ©Ã©s
    """
    try:
        # Lire le fichier
        with open(file_path, 'r', encoding='utf-8') as f:
            content = f.read()
        
        # Extraire le nom de la bibliothÃ¨que
        library_name = file_path.stem.replace('-context', '')
        
        # DÃ©couper en chunks
        chunks = text_splitter.split_text(content)
        
        # CrÃ©er les IDs, textes et mÃ©tadonnÃ©es
        ids = []
        texts = []
        metadatas = []
        
        for i, chunk in enumerate(chunks):
            chunk_id = f"{domain}_{library_name}_chunk_{i}"
            ids.append(chunk_id)
            texts.append(chunk)
            # MÃ©tadonnÃ©es MINIMALES pour rÃ©duire la taille
            metadatas.append({
                "domain": domain,
                "library": library_name,
                "chunk_index": i
            })
        
        # CrÃ©er les embeddings
        embeddings = embedding_model.encode(texts, show_progress_bar=False)
        
        # Ajouter Ã  ChromaDB par lots de 100
        batch_size = 100
        for i in range(0, len(ids), batch_size):
            batch_ids = ids[i:i+batch_size]
            batch_texts = texts[i:i+batch_size]
            batch_metadatas = metadatas[i:i+batch_size]
            batch_embeddings = embeddings[i:i+batch_size].tolist()
            
            collection.add(
                ids=batch_ids,
                documents=batch_texts,
                metadatas=batch_metadatas,
                embeddings=batch_embeddings
            )
        
        return len(chunks)
        
    except Exception as e:
        print(f" Erreur lors du traitement de {file_path.name}: {e}")
        return 0

# 5. Parcourir tous les domaines et fichiers
print("\n Traitement des contextes...")
print("=" * 60)

stats = {
    "total_files": 0,
    "total_chunks": 0,
    "by_domain": {}
}

domains = ["astronomy", "biochemistry", "finance", "machinelearning"]

for domain in domains:
    domain_path = CONTEXT_DIR / domain
    if not domain_path.exists():
        print(f"  Domaine non trouvÃ©: {domain}")
        continue
    
    context_files = list(domain_path.glob("*.txt"))
    stats["by_domain"][domain] = {
        "files": 0,
        "chunks": 0
    }
    
    print(f"\n Traitement du domaine: {domain.upper()}")
    print(f"   Fichiers trouvÃ©s: {len(context_files)}")
    
    for file_path in tqdm(context_files, desc=f"  {domain}"):
        num_chunks = process_context_file(file_path, domain)
        
        stats["total_files"] += 1
        stats["total_chunks"] += num_chunks
        stats["by_domain"][domain]["files"] += 1
        stats["by_domain"][domain]["chunks"] += num_chunks

# 6. Afficher les statistiques
print("\n" + "=" * 60)
print(" STATISTIQUES FINALES")
print("=" * 60)
print(f"\n Total de fichiers traitÃ©s: {stats['total_files']}")
print(f" Total de chunks crÃ©Ã©s: {stats['total_chunks']}")
print(f" Taille moyenne par chunk: {CHUNK_SIZE} caractÃ¨res")
print(f" Overlap: {CHUNK_OVERLAP} caractÃ¨res")

print("\nğŸ“ˆ Par domaine:")
for domain, data in stats["by_domain"].items():
    print(f"  â€¢ {domain:20s}: {data['files']:3d} fichiers â†’ {data['chunks']:6d} chunks")

# 7. Optimiser la base de donnÃ©es SQLite
print("\nğŸ—œï¸  Optimisation de la base de donnÃ©es...")
db_path = CHROMA_DIR / "chroma.sqlite3"

if db_path.exists():
    # Obtenir la taille avant optimisation
    size_before = db_path.stat().st_size / 1024 / 1024
    
    conn = sqlite3.connect(db_path)
    cursor = conn.cursor()
    
    # Supprimer le full-text search (inutile car on utilise les embeddings)
    try:
        cursor.execute("DROP TABLE IF EXISTS embedding_fulltext_search")
        cursor.execute("DROP TABLE IF EXISTS embedding_fulltext_search_data")
        cursor.execute("DROP TABLE IF EXISTS embedding_fulltext_search_idx")
        cursor.execute("DROP TABLE IF EXISTS embedding_fulltext_search_content")
        cursor.execute("DROP TABLE IF EXISTS embedding_fulltext_search_docsize")
        cursor.execute("DROP TABLE IF EXISTS embedding_fulltext_search_config")
        print("  âœ“ Full-text search supprimÃ© (inutile)")
    except Exception as e:
        print(f"  âš  Full-text search dÃ©jÃ  absent: {e}")
    
    # Compacter la base de donnÃ©es
    print("  â³ Compactage en cours...")
    cursor.execute("PRAGMA optimize")
    cursor.execute("VACUUM")
    cursor.execute("ANALYZE")
    
    conn.commit()
    conn.close()
    
    # Obtenir la taille aprÃ¨s optimisation
    size_after = db_path.stat().st_size / 1024 / 1024
    reduction = ((size_before - size_after) / size_before) * 100 if size_before > 0 else 0
    
    print(f"  âœ… Base de donnÃ©es optimisÃ©e:")
    print(f"     Avant:  {size_before:.2f} MB")
    print(f"     AprÃ¨s:  {size_after:.2f} MB")
    print(f"     Gain:   {reduction:.1f}%")

# 8. Sauvegarder les stats
stats_file = CHROMA_DIR / "indexing_stats.json"
with open(stats_file, 'w') as f:
    json.dump(stats, f, indent=2)

print(f"\nğŸ’¾ Statistiques sauvegardÃ©es: {stats_file}")
print("\nâœ… Indexation terminÃ©e avec succÃ¨s!")
print("=" * 60)

